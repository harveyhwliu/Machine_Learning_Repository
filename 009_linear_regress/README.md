## ***核心总结：***  

	1. 回归的依据 ： 目标值是连续的



  2.   画散点图：
	• import matplotlib.pyplot as plt
	• plt.figure(figsize=(10,10))
	• plt.scatter([60,72,75,80,83],[126,151.2,157.5,168,174.3])
	• plt.show()

  3.   线性关系的定义：
	• 分为：   线性组合  +  。。。
	• 线性回归：  二维空间，是一条直线，三维空间是一个平面上
	• 线性关系的定义：
		○ 单个特征 ：  y=kx +b   b 是偏置，为了使单个特征的情况更加通用
		○ 多个特征 ：  矩阵 Y = KX+B
		○ 线性关系模型 ：
			§ 一个通过属性(特征)的线性组合来进行预测的函数 ：  f(x)  = w1x1+w2x2+…+wdxd +b, w是权重，b为 偏置项，可以理解为： w0*1
		○    定义：
			§ 线性回归通过 一个  或  多个 自变量  与 因变量  之间的关系进行建模的回归分析。
			§ 一元线性回归：  涉及到的变量只有一个
			§ 多远线性回归：  涉及到的变量两个  或者  两个以上
			§ 通用公式：  h(w)  = w0+w1x1+w2x2 + …  = WTX, 其中W,X为矩阵，WT = [w0,w1,w2,…],XT   = [1,x1,x2,…]

   4. 数组     和 矩阵   的区别：
	• matrix是array的分支，matrix和array在很多时候都是通用的，你用哪一个都一样。但这时候，官方建议大家如果两个可以通用，那就选择array，因为array更灵活，速度更快，很多人把二维的   array也翻译成矩阵。
	•
	• 但是matrix的优势就是相对简单的运算符号，比如两个矩阵相乘，就是用符号*，但是array相乘不能这么用，得用方法.dot()
	• array的优势就是不仅仅表示二维，还能表示3、4、5...维，而且在大部分Python程序里，array也是更常用的

	• NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有：
		○ ndarray.ndim
数组轴的个数，在python的世界中，轴的个数被称作秩
		○ ndarray.shape
数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性
		○ ndarray.size
数组元素的总个数，等于shape属性中元组元素的乘积。
		○ ndarray.dtype
一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。
		○ ndarray.itemsize
数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8).
		○ ndarray.data
包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。
	• 矩阵（numpy中的矩阵必须是2维的）乘法： (m行，l列) * （l行，n列） = （m行，n列）
		○ 举例子： 【【1，2，3，4】】  *  [[2],[2],[2],[2]]        = 【【20】】   一个样本应该对应一个目标值
		○                     （1，4）                    （4，1）          =    （1，1）
		○                       特征值                        权重                         目标值
		>>> import numpy as np
		>>> a=[[1,2,3,4],[5,6,7,8],[2,3,7,9]]
		>>> b=[[2],[2],[2],[2]]
		>>> np.dot(a,b)     矩阵运算
		array([[20],
		       [52],
		       [42]])
		>>> b=[2,2,2,2]
		>>> np.multiply(a,b)   #数组运算  对每一列进行数据处理 * 一个算子
		array([[ 2,  4,  6,  8],
		       [10, 12, 14, 16],
		       [ 4,  6, 14, 18]])

		○
    5.  以后的算法都是：  迭代的算法   =》   迭代：（算法   + 策略 （损失函数）  +  优化  ）
	• 损失函数   ：误差最小
		○        yi  为  第i个训练样本的真实值
		○ hw(xi)  为第i个训练样本特征值组合预测函数
		○ 总损失定义 ：　　　　　
			§ J（θ）　＝　（hw(x１) －ｙ１）２　＋（hw(x２) －ｙ２）２＋．．．＋（hw(xｍ) －ｙｍｎ）２　＝　　∑（＋（hw(xi) －ｙi）２     　　　　
			§ 又称为　　　 最小二乘法（误差平方和）
			§
		○ 最小二乘法之 正规程法求解：（不要求掌握）
			§    求解  ：  W = （XTX）-1XTY     X为特征值矩阵，y是目标值矩阵
			§ 　缺点： 当特征过于复杂，求解速度太慢
		○ 最小二乘法 之  梯度下降（理解过程）：
			§ 以单变量中的w0,w1 为例子：
			§
			§ 理解：  沿着损失函数下降的方向找，最后就能找到山谷的最低点，然后更新w的值
			§ 使用：  面对训练数据规模十分庞大的任务
			§ cost : 损失函数   ，梯度下降：  学习率*方向
			§ 梯度下降： 初始方向是随机的




	• sklearn 线性回归  正规方程 和 梯度下降API
		○ 正规方程   sklearn.linear_model.LinearRegression
			§ sklearn.linear_model.LinearRegression()     普通最小二乘 线性回归
			§ coef_:   回归系数

		○ 梯度下降   sklearn.linear_model.SGDRegressor
			§ 通过使用SGD 最小化线性模型
			§ coef_:  回归系数




	• 算法流程：
		○ 数据集  ： 波士顿房价数据案例分析
		○ 分析流程：
			§ 波士顿地区房价数据获取   from sklearn.datasets import load_boston
			§ 波士顿地区房价数据分割
			§ 训练与测试数据标准化处理（另一个需要标准化处理的是KNN，但不同的是，线性回归需要对目标值也进行标准化处理）
			§ 使用最简单的线性回归模型LinearRegression 和  梯度下降估计 SGDRegressor  对房价进行预测
		○ 特别注意 ：
			§ sklearn   特征工程里面的转换器， 有的版本要求数据是二维数据  数组使用array.reshape(-1,1) 进行转换即可
		○ 回归性能的评估：  均方误差（Mean Squared Error）MSE 评价机制：
			§ MSE = 1/m  *(  ∑  （yi - y）)2
			§ yi是预测值 ， y是真实值
			§ sklearn 回归评估API：
				□ sklearn.metrics.mean_squared_error
				□ mean_squared_error(y_true,y_pred)
					® 均方误差回归损失
					® y_true: 真实值
					® y_pred:  预测值
					® return : 浮点数结果
					® 注意：  预测值  为标准化之前的值


	 1.  LinearRegression 与 SGDRegressor 评估

	2.  特点：
		a. 线性回归器 是最简单，易用的回归模型
		b. 从某种程度上限制了使用，尽管如此，在不知道特征之间的关系的前提下，仍然使用线性回归器作为大多数系统的首要选择。
		c. 小规模数据： linearRegression  (不能解决拟合问题)以及其他    容易产生过拟合现象
		d. 大规模数据： SGDRegressor


	3. 欠拟合 和  过拟合  ：

		a. 过拟合  ：  一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据，此时认为这个假设出现了过拟合现象。 （模型过于复杂）
			i. 解决办法： 原因  ： 原始特征过多，存在一些嘈杂特征，模型过于复杂是因为 模型尝试去兼顾各个测试点的数据
				1) 进行特征选择，消除关联性大的特征 （很难做）
				2) 交叉验证（让所有数据都有过训练）
				3) 正则化（了解）


		a. 欠拟合：  一个假设在训练数据上不能获得更好的拟合，但是在训练数据外的数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。（模型过于简单）
			i. 解决办法：    原因： 学习到数据的特征过少
				1) 增加数据的特征数量


		b. 模型复杂的原因  ：  数据的特征 和目标值  之间的关系  不仅仅是线性关系

	对 线性模型进行训练学习会变成复杂模型 ：


	解决过拟合作用：

		正则化：  尽量减少高次项特征的影响
		L2正则化：  Ridge:岭回归，带有正则化的线性回归，解决过拟合    （回归： 解决过拟合的方式）
			作用：  可以使得W的每个元素都很小，都接近于0
			优点： 越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象


	4. 带有正则化的线性回归  -Ridge  岭回归
		a. sklearn.linear_model.Ridge
		b. sklearn.linear_model.Ridge(alpha = 1.0)
			i. 具有l2正则化的线性最小二乘法
			ii. alpha ： 正则化粒度     （通常用 lamda）     0-1
			iii. coef_:    回归系数
			iv. 正则化粒度越来越大，模型越来越简单，高次项特征的权重 逐渐趋近为0 （不等于0？？？）
	5. 线性回归和岭回归对比
		a. 岭回归：   回归得到的回源系数更符合实际，更可靠。
		b. 另外，能让估计参数的波动范围变小，变得更稳定 。
		c. 在存在病态数据偏多的研究中   有较大的使用价值

	6. 总结：
		a. 数据集的划分  ：
			i. 训练集 和测试集   0.25
			ii. train_test_soplit :
				1) x_train , x_test
				2) y_train , y_test

		b. 转换器和 估计器   ：
			i. 转换器  ： 实例化
				1) fit_transform
				2) fit
				3) transform
			ii. 估计器：
				1) 实现了一类算法的API
				2) 流程   ：
					a) fit 训练数据
					b) predict  预测测试集的结果
					c) score 得出准确率
		c. 分类算法   ：
			i. K-近邻：
				1) 距离公式
				2) 优点：    理解简单   易于实现
				3) 缺点：
					a) K值取值问题
					b) 性能问题  ：   不适合用在  大量的数据集 中
				4) 超参数 ：   K值
				5) 数据处理   ：   必须需要标准化处理
			ii. 朴素贝叶斯算法   ：
				1) 概率基础 ：  条件概率     +  联合概率
				2) 朴素贝叶斯算法成立的前提条件   ：     条件独立
				3) 贝叶斯公式    ：   理解
				4) 优点  :  主要在  文本分类中应用 ，准确率比较高  ，   理论基础  是  概率
				5) 缺点   ：  条件独立    +    历史数据的准确性  （影响大 ）
				6) 数据处理   ：
					a) 文本的特征抽取
					b)
			iii. 决策树  ：
				1) 信息论   ：
					a) 信息熵
					b) 信息增益
					c) 信息熵的大小变化   是和   不确定性 相关的
				2) 分类依据   :
					a) 信息增益    （掌握   ）
					b) 信息增益比
					c) 基尼系数 （scikit  learn  默认）
				3)  优点 ：
					a) 准确率高
					b) 适用于各种数据
					c) 可解释性 强
				4) 缺点 ：
					a) 容易过拟合     树建的太深
			iv. 随机森林    ：
				1) 一种集成学习方法   ：   多个 同类的分类器组成
				2) 建立过程    ：
					a) 1
					b) 2
				3) 优点  ：
					a) 准确率高
					b) 不会过拟合
					c) 对大数据集 适用
					d)
				4) 超参数 ：
					a) 树的深度     多少棵树
			v. 分类算法的评估：
				1) 准确率
				2) 精确率  和  召回率
					a) 混淆矩阵
					b) 每一个类别都会  有
			vi. 模型调参数  ：
				1) 交叉验证       ：
					a) 为了让数据都能够进行验证和训练
					b) 训练数据   （训练验证  ）
					c) K折交叉验证
				2) 网格搜索
					a) 每个参数都会查看效果，选出效果好 的  参数
					b) 参数的组合


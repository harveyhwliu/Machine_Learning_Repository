## ***特征抽取***

### 特征抽取
   - 文本特征提取
       - 词袋（Bag of Words）表征
		  - 文本分析是机器学习算法的主要应用领域。但是，文本分析的原始数据无法直接丢给算法，这些原始数据是一组符号，因为大多数算法期望的输入是固定长度的数值特征向量而不是不同长度的文本文件。为了解决这个问题，scikit-learn提供了一些实用工具可以用最常见的方式从文本内容中抽取数值特征，比如说：
		  - 标记（tokenizing）文本以及为每一个可能的标记（token）分配的一个整型ID ，例如用白空格和标点符号作为标记的分割符（中文的话涉及到分词的问题）
		  - 计数（counting）标记在每个文本中的出现频率
		  - 正态化(nomalizating) 降低在大多数样本/文档中都出现的标记的权重
       - 在这个方案中，特征和样本的定义如下：
          - 将每个标记出现的频率(无论是否正态化)作为特征。
          - 给定文件中所有标记的出现频率所构成的向量作为多元样本。
          - 因此，语料文件可以用一个词文档矩阵代表，每行是一个文档，每列是一个标记（即词）。
          - 将文档文件转化为数值特征的一般过程被称为向量化。这个特殊的策略（标记，计数和正态化）被称为词袋或者Bag of n-grams表征。



   - 稀疏性
       - 大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。
       - 例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。
       - 为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征。



缺点：用词频描述文档，但是完全忽略词在文档中出现的相对位置信息。

### 特征抽取API
   - sklearn.feature_extraction  sklearn.feature_extraction.text
       - DictVectorizer   CountVectorizer
		  - .fit_transform(X)   X:字典或者包含字典的迭代器/文本或者包含文本字符串的可迭代对象，返回值 sparse矩阵
		  - .inverse_transform（X）  X:array 数组或者sparse矩阵，返回值 ： 转换之前数据格式
		  - .get_feature_names  返回类别名称
		  -  .transform         按照原先的标准转换

   - 流程：
      1. 实例化类 DictVecotrizer
      2. 调用fit_transform 方法 输入数据并转换   注意返回格式   One-hot编码

   - 字典特征值化：DictVectorizer
      1. 把字典中一些类别（key=类别）数据，分别进行转换成特征
      2. 数组形式，有类别的特征先转换为字典数据，再进行特征抽取

   - One-hot 编码：
      1. one hot编码是将  类别变量   转换为机器学习算法易于利用的一种形式的过程。  本质就是 进行 二进制编码
      1. 通过one-hot编码，可以对特征进行了扩充。
      2. 连续变量经过编码后，从一个权重变为多个权重，提升了模型的非线性能力。
      3. 不需要多参数进行归一化处理。
      4. 随着将大权重拆分成几个小权重管理特征，降低了异常值对模型的影响，增加了模型稳定性。
      5. 生成了较大的稀疏矩阵。

      6. 缺点： 如果原本的标签编码是有序的，那one hot编码就不合适了——会丢失顺序信息。

   - 文本特征抽取：
       - 对文本数据进行特征值化
       - Sklearn.feature_extraction.text.CountVectorizer

       - 1. 统计所有文章当中的所有的词，重复的只看做一次   词的列表
       - 2. 对每篇文章，在词的列表里面进行统计每个词出现的次数
       - 3. 单个字母不统计  ： 对文章分析认为没有影响

       - 4. 对中文文本特征抽取，默认是按照空格分割的
       - 5. 为了提高特征提取准确性，需要 分词
       - 6. 分词采用jieba 分词

	       -1. import jieba

	       -2. Jieba.cut( str_params )  #返回词语生成器

	       -3. 需要转换为所有词语的list   list(jieba.cut(str_params))

	       -4 用空格连接，产生新的字符串，待输入到.fit_transform(  )中进行分词  "".join( list(jieba.cut(str_params))）


       - 4. 应用： 文本分类 ， 情感分析


   - 词语占比问题： 如何分类文章的类型？？
       - 如果 两篇文章中  中性词过多，不能代表两篇文章类似


   - 5、tf    &  idf

       -主要思想： 如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合分类

       -作用： 用来评估一个词 对于 一个文件集或一个语料库中的其他一份文件的重要程度。

       -类： sklearn.feature_extraction.text.TfidfVectorizer

       -原理：

       -1. tf  term frequency : 词的频率

       -2. idf  inverse document frequency 逆文档频率     log(总文档数量/改词出现的文档数)

       -3. 计算指标 ： 重要性程度 ： tf*idf


## ***特征抽取***

### 特征抽取
   - 文本特征提取
       - 词袋（Bag of Words）表征
		  - 文本分析是机器学习算法的主要应用领域。但是，文本分析的原始数据无法直接丢给算法，这些原始数据是一组符号，因为大多数算法期望的输入是固定长度的数值特征向量而不是不同长度的文本文件。为了解决这个问题，scikit-learn提供了一些实用工具可以用最常见的方式从文本内容中抽取数值特征，比如说：
		  - 标记（tokenizing）文本以及为每一个可能的标记（token）分配的一个整型ID ，例如用白空格和标点符号作为标记的分割符（中文的话涉及到分词的问题）
		  - 计数（counting）标记在每个文本中的出现频率
		  - 正态化(nomalizating) 降低在大多数样本/文档中都出现的标记的权重
       - 在这个方案中，特征和样本的定义如下：
          - 将每个标记出现的频率(无论是否正态化)作为特征。
          - 给定文件中所有标记的出现频率所构成的向量作为多元样本。
          - 因此，语料文件可以用一个词文档矩阵代表，每行是一个文档，每列是一个标记（即词）。
          - 将文档文件转化为数值特征的一般过程被称为向量化。这个特殊的策略（标记，计数和正态化）被称为词袋或者Bag of n-grams表征。



   - 稀疏性
       - 大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。
       - 例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。
       - 为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征。



缺点：用词频描述文档，但是完全忽略词在文档中出现的相对位置信息。

### 特征抽取API
   - sklearn.feature_extraction  sklearn.feature_extraction.text
       - DictVectorizer   CountVectorizer
		  - .fit_transform(X)   X:字典或者包含字典的迭代器/文本或者包含文本字符串的可迭代对象，返回值 sparse矩阵
		  - .inverse_transform（X）  X:array 数组或者sparse矩阵，返回值 ： 转换之前数据格式
		  - .get_feature_names  返回类别名称
		  -  .transform         按照原先的标准转换

   - 流程：
      1. 实例化类 DictVecotrizer
      2. 调用fit_transform 方法 输入数据并转换   注意返回格式   One-hot编码

   - 字典特征值化：DictVectorizer
      1. 把字典中一些类别（key=类别）数据，分别进行转换成特征
      2. 数组形式，有类别的特征先转换为字典数据，再进行特征抽取

   - One-hot 编码：
      1. one hot编码是将  类别变量   转换为机器学习算法易于利用的一种形式的过程。  本质就是 进行 二进制编码
      1. 通过one-hot编码，可以对特征进行了扩充。
      2. 连续变量经过编码后，从一个权重变为多个权重，提升了模型的非线性能力。
      3. 不需要多参数进行归一化处理。
      4. 随着将大权重拆分成几个小权重管理特征，降低了异常值对模型的影响，增加了模型稳定性。
      5. 生成了较大的稀疏矩阵。

      6. 缺点： 如果原本的标签编码是有序的，那one hot编码就不合适了——会丢失顺序信息。

   - 文本特征抽取：
       - 对文本数据进行特征值化
       - Sklearn.feature_extraction.text.CountVectorizer

       - 1. 统计所有文章当中的所有的词，重复的只看做一次   词的列表
       - 2. 对每篇文章，在词的列表里面进行统计每个词出现的次数
       - 3. 单个字母不统计  ： 对文章分析认为没有影响

       - 4. 对中文文本特征抽取，默认是按照空格分割的
       - 5. 为了提高特征提取准确性，需要 分词
       - 6. 分词采用jieba 分词

	       -1. import jieba

	       -2. Jieba.cut( str_params )  #返回词语生成器

	       -3. 需要转换为所有词语的list   list(jieba.cut(str_params))

	       -4 用空格连接，产生新的字符串，待输入到.fit_transform(  )中进行分词  "".join( list(jieba.cut(str_params))）


       - 4. 应用： 文本分类 ， 情感分析


   - 词语占比问题： 如何分类文章的类型？？
       - 如果 两篇文章中  中性词过多，不能代表两篇文章类似


   - 5、tf    &  idf

       -主要思想： 如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合分类

       -作用： 用来评估一个词 对于 一个文件集或一个语料库中的其他一份文件的重要程度。

       -类： sklearn.feature_extraction.text.TfidfVectorizer

       -原理：

       -1. tf  term frequency : 词的频率

       -2. idf  inverse document frequency 逆文档频率     log(总文档数量/改词出现的文档数)

       -3. 计算指标 ： 重要性程度 ： tf*idf


### 数据的类型

	- 1、 对数据进行预处理   ： 提供 特定统计方法（数据方法） 将数据转换成 算法要求的数据

		- 1） 特征预处理的方法  ：

			- 1、 数值型数据  ：标准缩放

				- 1、归一化

					- 1）特点： 通过对原始数据进行变换，将  数据映射到 默认为【0-1 】之间

					- 2）应用场景：各个数据特征是同等重要时，需要进行归一化操作

					- 3）公式：

						X'\quad =\quad \frac { x-min }{ max\quad -\quad min } \\ X''\quad =\quad X'\quad *\quad （ma\quad -\quad mi)\quad +mi


						备注：作用于每一列，max为一列的最大值，min 为一列的最小值，那么X"为最终结果，mx ，mi 为指定区间，默认 mx = 1,mi = 0

					- 4）Sklearn API：

						- Sklearn.MinMaxScaler

						- MinMaxScaler(feature_range=(1,2))   #通过指定feature_range指定归一化后值域被映射到【1-2】，默认是[0-1]

					- 5）弊端：

						- 如果数据中有明显的异常点 且 较多， 异常点  对 最大值，最小值影响较大

						- 归一化  对异常点的影响较差

					- 6）总结：

						- 在特定场景下最大值 最小值是变化的，另外，最大值和最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。
				- 2、标准化

					- 1） 特点：通过对原始数据进行变化，把数据变化到 均值为0 标准差为1的范围内

					- 2）应用场景：

						- 在已有样本足够多的场景下比较稳定，适合于现代嘈杂大数据场景

					- 3）公式：X'=\frac { x-mean }{ deta }

					- 4）x’= （x-mean）/deta 作用于每一列，mean为平均值，deta 为标准差，var 是方差

					- 5）对于归一化来说，如果出现异常点，影响了最大值和最小值，那么结果显然会发生变化

					- 6）对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于均值的影响并不大，从而方差改变小

					- 7）sklearn特征化API，sklearn.preprocessing.StandardScaler

						- .fit_transform()

						- .scale_

						- .mean_

				- 3、缺失值

					- 1） 处理思想：

						- 删除：如果每列或者行数据缺失值达到一定的比例，建议放弃正行或者整列

						- 插补：可以通过缺失值 每行或者每列的平均值，中位数来填充

					- 2）API：

						- sklearn缺失值API： sklearn.preprocessing.Imputer

						- Imputer(missing_values='NaN',strategy='mean',axis=0) 完成缺失值插补  axis =0 指定的是行 或者列

						- 处理流程：

							- 1） 初始化Imputer 指定的“缺失值”，指定填补策略，指定行或者列     注意：缺失值也可以是别的指定要替换的值

							- 2）调用fit_transform

					- 3） 关于 np.nan(np.NaN):

						- 1、numpy 的数组可以使用np.nan/np.NaN来代替缺失值，属于float类型

						- 2、如果是文件中的一些缺失值，可以替换成nan,通过np.array 转化成float 型的数组即可。


					- 4）Pandas 处理 ：

						- 前提是数据当中的确实值是：np.nan

						- Pandas:dropna 删除

						- Pandas:Fillna 插补

						- Pandas::replace("?",np.nan)

			- 2、类别型数据： one-hot 编码

			- 3、时间类型：
				- 时间且切分

		- 2） sklearn 特征预处理的API
            - Skearn.preprocessing


五、机器学习算法基础
	1）算法是核心，数据和计算是基础。
	2） 找准定位：
		大部分复杂模型的算法设计都是算法工程师在做，而我们在做：
			分析很多的数据
			分析具体的业务
			应用常见的算法
			特征工程，调参数，优化
	3）我们应该怎么做？
		学会分析问题，使用机器学习算法的目的，想要算法完成何种任务
		掌握算法的基本思想，学会对问题用相应的算法解决
		学会利用库或者框架解决问题

	4） 数据类型：
		离散型数据：
			又称：计算数据，全部都是整数，而且不能再细分，也不能进一步提高他们的精度。

		连续型数据：
			再某个范围内取值任意数，取值是连续的，如长度，时间，质量值等，这类数据通常是非整数，含有小数部分。

		本质区别是一点：
			离散型数据 是区间内不可分，连续型数据是区间内可分的



	开发流程：
		0、公司本身是有数据的
		0、公司本身是没有数据的（合作过来的数据，购买过来的数据）
		1、明确为的是什么？目的是干什么？  --  建立模型（根据标签值/目标值划分应用种类）
		2、数据的基本处理：pandas 处理 数据（缺失值，标准化，合并表等）
		3、特征工程（特征进行处理）  （重要环节）
		4、找到合适的算法进行预测
		5、模型的评估：  准确率提升等
		6、上线使用，以API形式提供  或者 确定模型

		7、如果步骤5没有合格，换算法，调参数，甚至回到特征工程，循环流程。

		模型 ： 算法+数据



	算法分类：



		监督学习（预测）：  特征值 + 目标值（标签值）
			分类：  离散型目标值
				 k-近邻算法，贝叶斯分类，决策树与随机森林，逻辑回归，神经网络

			回归： 连续型目标值
				  线性回归，岭回归

			标注： 隐马尔科夫模型（不做要求）

		无监督学习（聚类）：特征值
			聚类  k-me ans


	机器学习模型：
